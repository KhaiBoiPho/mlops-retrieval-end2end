# Bi-Encoder Serving Configuration

# Model configuration
model:
  # S3 configuration
  s3_bucket: "khai-bucket-s3-mlflow"
  model_id: "eada2728338848f987d12d4fa2a0b77b"  # Specific model version, or "latest"
  model_type: "bi-encoder"  # bi-encoder or cross-encoder
  local_path: "artifacts/bi-encoder"  # Model path (will be downloaded to this location)

# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1  # RunPod serverless typically uses 1 worker
  reload: false

# Inference settings
inference:
  batch_size: 32
  max_seq_length: 256
  device: "cuda"  # cuda/cpu, auto-detected if not specified
  auto_tokenize: true

# Cache settings (optional, for future use)
cache:
  enable: false
  ttl: 3600  # 1 hour
  max_size: 10000