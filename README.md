# MLOps Retrieval End-to-End

A production-grade MLOps system for Vietnamese legal document retrieval implementing bi-encoder and cross-encoder architecture with complete MLOps lifecycle.

## ğŸ¯ Overview

End-to-end retrieval system built for **SoICT Hackathon 2024 Document Retrieval Challenge**, featuring:

- **DVC Pipeline**: Versioned data processing and validation
- **Dual-Model Architecture**: Bi-encoder (retrieval) + Cross-encoder (reranking)
- **Airflow Orchestration**: Automated training workflow
- **MLflow Tracking**: Experiment management and model registry
- **RunPod Serverless**: Queue-based inference deployment
- **Full Observability**: Prometheus + Grafana + Loki + Telegram alerts

## ğŸ—ï¸ System Architecture

```
S3 Raw Data â†’ DVC Pipeline â†’ Airflow DAGs â†’ Training (Bi + Cross Encoder)
    â†“
MLflow Registry â†’ S3 Model Storage â†’ RunPod Serverless Endpoints
    â†“
Prometheus â† Service Metrics â†’ Grafana Dashboards â†’ Alertmanager â†’ Telegram
    â†‘
Loki Logs â† Promtail
```

## ğŸ“ Project Structure

```
mlops-retrieval-end2end/
â”œâ”€â”€ airflow/                          # Workflow orchestration
â”‚   â”œâ”€â”€ dags/                         # Training pipeline DAGs
â”‚   â””â”€â”€ logs/                         # Execution logs
â”‚
â”œâ”€â”€ artifacts/                        # Training outputs
â”‚   â”œâ”€â”€ bi-encoder/                   # Model checkpoints & eval results
â”‚   â”œâ”€â”€ cross-encoder/
â”‚   â””â”€â”€ corpus_embeddings/            # Pre-computed embeddings
â”‚
â”œâ”€â”€ ci/                               # Continuous Integration scripts
â”‚   â”œâ”€â”€ build_*.sh                    # Docker builds
â”‚   â”œâ”€â”€ lint.sh                       # Code quality checks
â”‚   â””â”€â”€ test.sh                       # Unit & integration tests
â”‚
â”œâ”€â”€ cd/                               # Continuous Deployment scripts
â”‚   â”œâ”€â”€ deploy_*.sh                   # Deployment automation
â”‚   â”œâ”€â”€ health_check.sh
â”‚   â”œâ”€â”€ canary_deploy.sh
â”‚   â””â”€â”€ rollback.sh
â”‚
â”œâ”€â”€ config/                           # All configurations
â”‚   â”œâ”€â”€ bi-encoder/                   # Train, eval, serve configs
â”‚   â”œâ”€â”€ cross-encoder/
â”‚   â”œâ”€â”€ data.yaml
â”‚   â”œâ”€â”€ monitoring.yaml
â”‚   â””â”€â”€ rollback.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Original S3 data
â”‚   â”œâ”€â”€ processed/                    # Cleaned data
â”‚   â””â”€â”€ use/                          # Training-ready splits
â”‚
â”œâ”€â”€ docker/                           # Deployment containers
â”‚   â”œâ”€â”€ bi_encoder/Dockerfile.serverless
â”‚   â””â”€â”€ cross_encoder/Dockerfile.serverless
â”‚
â”œâ”€â”€ monitoring/                       # Observability stack
â”‚   â”œâ”€â”€ prometheus/                   # Alert rules & recording rules
â”‚   â””â”€â”€ grafana/dashboards/           # 3 dashboards (pipeline, bi, cross)
â”‚
â”œâ”€â”€ reports/                          # Evaluation artifacts
â”‚   â”œâ”€â”€ bi-encoder/                   # metrics.json, predictions.csv
â”‚   â””â”€â”€ cross-encoder/
â”‚
â”œâ”€â”€ scripts/                          # Utility scripts
â”‚   â”œâ”€â”€ download_checkpoints_from_s3.py
â”‚   â””â”€â”€ update_s3_latest.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/                       # Shared utilities
â”‚   â”‚   â”œâ”€â”€ s3_utils.py               # S3 operations
â”‚   â”‚   â”œâ”€â”€ configuration.py          # Config loader
â”‚   â”‚   â”œâ”€â”€ notify/telegram.py        # Telegram bot
â”‚   â”‚   â””â”€â”€ model_gates/              # Model validation
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                         # Data pipeline
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py         # S3 download
â”‚   â”‚   â”œâ”€â”€ processors.py             # PyVi segmentation + cleaning
â”‚   â”‚   â””â”€â”€ pipeline.py               # DVC pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                     # Feature engineering
â”‚   â”‚   â”œâ”€â”€ build_corpus_embeddings.py
â”‚   â”‚   â””â”€â”€ hard_negative_mining.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                       # Model implementations
â”‚   â”‚   â”œâ”€â”€ bi_encoder/               # BKAI wrapper + custom loss/pooling
â”‚   â”‚   â””â”€â”€ cross_encoder/            # PhoRanker wrapper + BCE loss
â”‚   â”‚
â”‚   â”œâ”€â”€ train/                        # Training logic
â”‚   â”‚   â”œâ”€â”€ bi_encoder/trainer.py
â”‚   â”‚   â”œâ”€â”€ cross_encoder/trainer.py
â”‚   â”‚   â””â”€â”€ callbacks.py
â”‚   â”‚
â”‚   â”œâ”€â”€ eval/                         # Evaluation
â”‚   â”‚   â”œâ”€â”€ bi_encoder_evaluator.py
â”‚   â”‚   â”œâ”€â”€ cross_encoder_evaluator.py
â”‚   â”‚   â””â”€â”€ metrics/                  # Custom PyTorch metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ registry/                     # MLflow integration
â”‚   â”‚   â”œâ”€â”€ mlflow_client.py
â”‚   â”‚   â”œâ”€â”€ model_registry.py
â”‚   â”‚   â””â”€â”€ promote.py
â”‚   â”‚
â”‚   â”œâ”€â”€ serve/                        # Serving layer
â”‚   â”‚   â”œâ”€â”€ bi_encoder_service/       # FastAPI + RunPod handler
â”‚   â”‚   â””â”€â”€ cross_encoder_service/
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/                   # Metrics instrumentation
â”‚   â”‚   â”œâ”€â”€ performance_tracker.py
â”‚   â”‚   â”œâ”€â”€ embedding_drift.py
â”‚   â”‚   â””â”€â”€ alert_rules.py
â”‚   â”‚
â”‚   â””â”€â”€ retraining/                   # Auto-retrain logic
â”‚       â”œâ”€â”€ policy.py
â”‚       â””â”€â”€ trigger.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”‚
â”œâ”€â”€ dvc.yaml                          # DVC pipeline definition
â””â”€â”€ requirements.txt
```

## ğŸ”„ Data Pipeline

### Pipeline Stages (DVC-tracked)

1. **Data Ingestion**: Download `train.csv` and `corpus.csv` from S3
2. **Preprocessing**: Vietnamese word segmentation (PyVi) + text cleaning
3. **Data Splitting**: Train (70%) / Val (15%) / Test (15%)
4. **Validation**: Schema checks and quality gates

All transformations are versioned with DVC and stored on S3.

## ğŸš‚ Training Pipeline

### Airflow DAG Flow

```
download_data â†’ preprocess â†’ word_segment â†’ data_split â†’ validate
    â†“
train_bi_encoder â†’ eval_bi_encoder â†’ save_bi_encoder
    â†“
build_corpus_embeddings â†’ mine_hard_negatives
    â†“
train_cross_encoder â†’ eval_cross_encoder â†’ save_cross_encoder
    â†“
notify_telegram
```
<table align="center">
  <tr>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-56-08.png" width="700">
    </td>
    <td align="center">
      <img src="images/download.jpeg" width="180">
    </td>
  </tr>
</table>

<p align="center">
  <i>End-to-end training pipeline orchestrated by Airflow DAGs</i>
</p>


### Bi-Encoder Training

- **Base Model**: `bkai-foundation-models/vietnamese-bi-encoder`
- **Loss**: MultipleNegativesRankingLoss / CachedMultipleNegativesRankingLoss (custom PyTorch)
- **Custom Components**: Pooling layers (mean/CLS/max) implemented from scratch
- **Evaluation Metrics**: Recall@k, MRR@k, nDCG@k, MAP@k, Hit Rate@k
- **Output**: Checkpoints, best model, eval reports (metrics.json, predictions.csv, failed_cases.csv)

### Hard Negative Mining

Uses trained bi-encoder to generate challenging negative samples for cross-encoder training.

### Cross-Encoder Training

- **Base Model**: `itdainb/PhoRanker`
- **Loss**: Binary Cross-Entropy (custom PyTorch)
- **Evaluation Metrics**: Precision@k, Reranking Accuracy, MRR Improvement, RRD, nDCG Improvement
- **Output**: Same structure as bi-encoder

### MLflow Integration

- Tracks all hyperparameters and metrics
- Registers models with unique run IDs
- Version tagging (staging/production)
- Used for model retrieval during deployment

<p align="center">
    <p align="center">
      <img src="images/Screenshot from 2026-01-14 00-58-10.png" width="900">
    </p>
<p align="center"><i>MLflow Training Metrics & Artifacts</i></p>

### Model Storage

Models stored on S3 with structure:
```
s3://mlops-retrieval-models/
â”œâ”€â”€ bi-encoder/{mlflow_run_id}/
â””â”€â”€ cross-encoder/{mlflow_run_id}/
```

### Telegram Notifications

After training completion, sends summary with:
- Final metrics (Recall@10, MRR, nDCG, etc.)
- MLflow run IDs
- S3 paths
- Training duration

## ğŸš€ Deployment

### RunPod Serverless Architecture

**Queue-Based System** (no persistent workers):
- Workers spin up on-demand (cold start)
- Auto-scale to zero when idle
- Pay-per-second billing
- Cost-efficient for variable traffic

### Deployment Process

1. Update `config/bi-encoder/serve.yaml` and `config/cross-encoder/serve.yaml` with trained model run IDs
2. Models downloaded from S3 using MLflow run IDs
3. Docker images built for both services
4. RunPod handlers wrap FastAPI services
5. Deploy to serverless endpoints
6. Health checks validate deployment

### Service Architecture

Each service exposes:
- `/health` - Health check endpoint
- `/infer` - Inference endpoint
- `/metrics` - Prometheus metrics

<p align="center">

<table align="center">
  <tr>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-33-26.png" width="600">
    </td>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-33-12.png" width="600">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-42-52.png" width="600">
    </td>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-43-13.png" width="600">
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-44-39.png" width="600">
    </td>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-44-52.png" width="600">
    </td>
  </tr>
</table>

<p align="center">
  <i>RunPod serverless inference endpoints for bi-encoder and cross-encoder models</i>
</p>


## ğŸ“ˆ Monitoring System

### Four Pillars of ML Observability

#### 1. Reliability
- `endpoint_requests_total` - Total requests by endpoint/action/status
- `endpoint_cold_starts_total` - Cold start tracking
- `endpoint_errors_total` - Error breakdown by type

#### 2. Performance
- `endpoint_request_duration_seconds` - End-to-end latency (histogram)
- `endpoint_inference_duration_seconds` - Model inference time
- `endpoint_batch_size` - Batch size distribution

#### 3. Model Quality
- **Bi-Encoder**: `endpoint_embedding_dimension` (drift detection)
- **Cross-Encoder**: `endpoint_score_value`, `score_min/max` (distribution monitoring)

#### 4. Cost
- `endpoint_cost_per_request` - Real-time cost per request
- `endpoint_cost_per_1k_requests` - Aggregated cost metrics

**Cost calculated in-code during inference, not estimated post-facto.**

### Monitoring Stack

```
Service (instrumented) â†’ /metrics endpoint â†’ Ngrok (public) 
    â†’ Prometheus (scrape + rules) â†’ Grafana (dashboards)
    â†’ Alertmanager â†’ Telegram
```

<table align="center">
  <tr>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-35-51.png" width="380"><br/>
      <b>Prometheus</b>
    </td>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-36-12.png" width="380"><br/>
      <b>Grafana</b>
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-36-40.png" width="380"><br/>
      <b>Pushgateway</b>
    </td>
    <td align="center">
      <img src="images/Screenshot from 2026-01-14 00-35-27.png" width="380"><br/>
      <b>Alertmanager</b>
    </td>
  </tr>
</table>

<p align="center">
  <i>Core monitoring components for metrics collection, visualization, and alerting</i>
</p>


Logs collected via Promtail â†’ Loki â†’ displayed in Grafana alongside metrics.

### Dashboards

1. **Pipeline Dashboard**: Training progress, Airflow tasks, MLflow experiments
2. **Bi-Encoder Dashboard**: Success rate, latency P95, cold starts, embedding drift, cost
3. **Cross-Encoder Dashboard**: Reranking latency, score distribution, drift detection

### Alerting

Prometheus evaluates rules and triggers alerts for:
- High error rate (>5%)
- High latency (P95 > 2s)
- Cold start spikes
- Embedding dimension changes (silent model swap)
- Score distribution drift

Alerts sent to Telegram with dashboard links and actionable context.

### Philosophy

> "No magic, just discipline: metrics measured in code â†’ ngrok tunnel â†’ Prometheus calculates â†’ Grafana visualizes â†’ Alertmanager notifies."

## ğŸ”„ CI/CD Pipeline

### Continuous Integration
- **Linting**: black, flake8, mypy
- **Testing**: pytest (unit + integration)
- **Build**: Docker image validation

### Continuous Deployment
- **Standard Deploy**: Build â†’ Push â†’ Deploy â†’ Health Check
- **Canary Deploy**: 10% traffic â†’ monitor â†’ promote or rollback
- **Rollback**: Revert to previous version stored in config

All scripts in `ci/` and `cd/` directories.

## ğŸš€ Quick Start

### Prerequisites
```bash
python >= 3.9, docker, aws-cli, dvc, airflow
```

### Installation
```bash
git clone <repo>
cd mlops-retrieval-end2end
pip install -r requirements.txt
aws configure
dvc remote add -d s3remote s3://mlops-retrieval-data
dvc pull
```

### Run Training
```bash
# Start Airflow
export AIRFLOW_HOME=$(pwd)/airflow
airflow db init
airflow webserver -p 8080 &
airflow scheduler &

# Trigger pipeline
airflow dags trigger legal_retrieval_training_pipeline
```

### Deploy
```bash
# Update configs with trained model IDs
vim config/*/serve.yaml

# Build and deploy
bash ci/build_bi_encoder.sh
bash ci/build_cross_encoder.sh
bash cd/deploy_pipeline.sh
```

### Monitor
```bash
# Start monitoring stack
prometheus --config.file=monitoring/prometheus/prometheus.yml &
grafana-server &

# Access Grafana at http://localhost:3000
# Import dashboards from monitoring/grafana/dashboards/
```

## ğŸ“Š Key Features

âœ… **Data Versioning**: DVC tracks all transformations  
âœ… **Experiment Tracking**: MLflow logs everything  
âœ… **Custom PyTorch**: All losses, pooling, metrics from scratch  
âœ… **Automated Orchestration**: Airflow manages workflow  
âœ… **Serverless Deployment**: RunPod queue-based architecture  
âœ… **Full Observability**: 4-pillar monitoring with real-time alerting  
âœ… **CI/CD**: Automated testing, building, deploying, rollback  
âœ… **Cost Tracking**: In-code cost calculation per request  

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Add tests and documentation
4. Run `bash ci/lint.sh` and `bash ci/test.sh`
5. Submit Pull Request

## ğŸ“ License

MIT License

## ğŸ“§ Contact

For questions or issues, open a GitHub issue or contact via Telegram.

---

**Built for Vietnamese Legal Document Retrieval**
